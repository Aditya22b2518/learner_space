import zipfile
import os
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras import Input

# Path to the ZIP file in Colab
zip_path = '/content/homer_bart.zip'

# Directory where images will be extracted
extract_path = '/content/homer_bart'

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Directory where images are extracted
data_dir = extract_path

# Parameters
batch_size = 32
img_height = 64
img_width = 64

# Load the dataset
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    image_size=(img_height, img_width),
    label_mode="binary",
    batch_size=batch_size,
    shuffle=True,
    seed=42
)

# Split the dataset
total_batches = tf.data.experimental.cardinality(dataset).numpy()
train_batches = int(0.9 * total_batches)
test_batches = total_batches - train_batches

train_data = dataset.take(train_batches)
test_data = dataset.skip(train_batches)

train_data = train_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_data = test_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

# Preprocessing layer
preprocess = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255)
])

# Define the model
model = tf.keras.Sequential()
model.add(Input((img_height, img_width, 3)))
model.add(preprocess)
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
              metrics=['accuracy'])

# Train the model
model.fit(train_data,
          epochs=40,
          batch_size=batch_size,
          verbose=1,
          validation_data=test_data)

# Evaluate the model
loss, accuracy = model.evaluate(test_data)
print(f'Test Accuracy: {accuracy:.4f}')
